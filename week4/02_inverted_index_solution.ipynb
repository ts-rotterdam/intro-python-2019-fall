{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple search engine\n",
    "## Part 2: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will build a inverted search index, and then search the dataset using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "For this exercise we will be loading both files from the dataset and generate a named tuple also containing the descriptions. This may take roughly a minute depending on the computing power of your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle, lzma\n",
    "from collections import namedtuple\n",
    "\n",
    "summaries_file = 'data/arxiv_cs_summaries.pickle.xz'\n",
    "descriptions_file = 'data/arxiv_cs_descriptions.pickle.xz'\n",
    "\n",
    "summaries = pickle.load(lzma.LZMAFile(summaries_file, 'rb'))\n",
    "descriptions = pickle.load(lzma.LZMAFile(descriptions_file, 'rb'))\n",
    "\n",
    "paper = namedtuple(\"paper\", [\"title\", \"authors\", \"year\", \"link\", \"description\"])\n",
    "papers = [paper(*summaries[i], descriptions[i]) for i in range(len(summaries))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper(title='Intelligent location of simultaneously active acoustic emission sources:  Part I', authors=['Kosel, T.', 'Grabec, I.'], year=2007, link='http://arxiv.org/abs/0704.0047', description='The intelligent acoustic emission locator is described in Part I, while Part II discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.   The location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. This article describes an intelligent acoustic emission source locator. The intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. Locator performance was tested on different test specimens. Tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. The location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. This is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.')\n"
     ]
    }
   ],
   "source": [
    "print(papers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The code below defines a function `print_paper(paper_id, print_description=False, highlight=[])` to print the summary of a paper (with descriptions) given its id, and optionally highlight parts of the description given a list of strings. You can use this to quickly test your search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_paper(paper_id, print_description=False, highlight=[]):\n",
    "    p = papers[paper_id]\n",
    "    display(HTML(f\"<b><a href='{p.link}'>{p.title}</a></b><br>{p.year} - {', '.join(p.authors)}\"))\n",
    "    if print_description:\n",
    "        description = descriptions[paper_id]\n",
    "        for h in highlight:\n",
    "            description = description.replace(h, f\"<span style='background: yellow'>{h}</span>\")\n",
    "        display(HTML(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0047'>Intelligent location of simultaneously active acoustic emission sources:  Part I</a></b><br>2007 - Kosel, T., Grabec, I."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0050'>Intelligent location of simultaneously active acoustic emission sources:  Part II</a></b><br>2007 - Kosel, T., Grabec, I."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0304'>The World as Evolving Information</a></b><br>2010 - Gershenson, Carlos"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in range(3):\n",
    "    print_paper(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index\n",
    "\n",
    "What is the most basic problem a search engine has to solve? Given a search term as input, output a list of results containing the search term. More specifically, given a `string` as input, return a list of papers whose descriptions contain the `string`. So how do we do that?\n",
    "\n",
    "\"The answer is quite straightforward\", you say, \"just loop over the papers and if the paper contain the result, put it in a list, something like this\":\n",
    "\n",
    "```\n",
    "def search(query):\n",
    "    return [p for p in papers if query in p.description]\n",
    "```\n",
    "\n",
    "The function above would indeed return a list of papers with the results, but there are a few problems:\n",
    "- This function only returns exact matches of your search query, i.e. `\"Computer\" != \"computer\" != \"computers\"`\n",
    "- The search function takes a while to run (and will take even longer as the dataset becomes bigger)\n",
    "\n",
    "The above reason (mostly the performance one) is why we would use an inverted index. First let's talk about the index we have learned about. We have a list of tuples, and each tuple contains the properties of a paper (title, authors, year, link, description). This list can be treated as an index: we can get one paper using its position (index) in the list.\n",
    "\n",
    "An inverted index is created differently: we create an index based on the **terms** (similar to words) within the descriptions. Instead of getting one paper by its position (i.e. `papers[12345]` returns a `tuple`), we get a list of papers containing a term by the term itself (i.e. `index[\"computer\"]` returns a `list` of papers containing the term `\"computer\"` in the description).\n",
    "\n",
    "To create an inverted index, first we process the descriptions into a list of terms, and then for each of those terms we create a set of articles containing that word. If we rephrase it into Python terms, we are creating a `dict` whose keys are the terms (`string`) and the results are a `set` of indices (`int`) of the articles whose description contains that term, for example:\n",
    "```\n",
    "{\n",
    "    \"computer\": {1, 2, 3, 4, 5},\n",
    "    \"network\": {2, 5, 8, 9},\n",
    "    \"term\": {1, 11},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text\n",
    "\n",
    "Two of the common steps in producing a list of **terms** from a text: **tokenization** and **stemming**.\n",
    "\n",
    "#### Tokenization\n",
    "Tokenization breaks a body of text into individual semantic units called **tokens**, for example:\n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "becomes\n",
    "```\n",
    "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n",
    "```\n",
    "\n",
    "#### Stemming\n",
    "Stemming reduces/normalises tokens to a base *stem* so that we don't need an exact match for the same word in another form, for example *is*, *am*, *are* can be reduced to *be*, and in the above sentence *The* can be reduced to *the*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, we will tokenize the paper descriptions by splitting the text by space, and then stem the tokens by converting them into lower case. Let's define the two functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split(\" \")\n",
    "\n",
    "def stem(tokens):\n",
    "    return [t.lower() for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# A defaultdict creates a set by defualt when we save items to a new key\n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "# This may take a while\n",
    "for (i, p) in enumerate(papers):\n",
    "    for term in stem(tokenize(p.description)):\n",
    "        inverted_index[term].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the index\n",
    "Now that we have created the index let's test it with some queries. The code below should return 38618 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0047'>Intelligent location of simultaneously active acoustic emission sources:  Part I</a></b><br>2007 - Kosel, T., Grabec, I."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0671'>Learning from compressed observations</a></b><br>2007 - Raginsky, Maxim"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.1028'>A neural network approach to ordinal regression</a></b><br>2007 - Cheng, Jianlin"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.2668'>Supervised Feature Selection via Dependence Estimation</a></b><br>2007 - Song, Le, Smola, Alex, Gretton, Arthur, Borgwardt, Karsten, Bedo, Justin"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.3453'>An Adaptive Strategy for the Classification of G-Protein Coupled  Receptors</a></b><br>2007 - Mohamed, S., Rubin, D., Marwala, T."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"learning\"\n",
    "\n",
    "# display(HTML(f\"<h3>There are {len(inverted_index[query])} results for '{query}':</h3><hr>\"))\n",
    "\n",
    "for i in list(inverted_index[query])[:5]:\n",
    "    print_paper(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverted index we created above can be searched by a single word, but we rarely want to search with only one term. To find documents containing multiple search terms, we can look into the sets of papers for each search term and find the intersection of all of them.\n",
    "\n",
    "**Exercise:** Define a function `and_query(query)` that returns a set of articles that match all input terms. The query for `machine learning` should return 7944 results.\n",
    "\n",
    "*Hint:* Tokenize and stem your query first using `tokenize()` and `stem()`. The results in the inverted index are `set`s, which means you can perform set operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_query(query):\n",
    "    terms = stem(tokenize(query))\n",
    "    # Find the intersection of the terms and return it\n",
    "    results = inverted_index[terms[0]]\n",
    "    for term in terms[1:]:\n",
    "        results = results.intersection(inverted_index[term])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>There are 7944 results for 'machine learning':</h3><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/1707.04943'>Improving Naive Bayes for Regression with Optimised Artificial Surrogate  Data</a></b><br>2018 - Mayo, Michael, Frank, Eibe"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/1904.10400'>Efficient single input-output layer spiking neural classifier with  time-varying weight model</a></b><br>2019 - Jeyasothy, Abeegithan, Ramasamy, Savitha, Sundaram, Suresh"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/1904.10416'>Regression-Enhanced Random Forests</a></b><br>2019 - Zhang, Haozhe, Nettleton, Dan, Zhu, Zhengyuan"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/1904.10504'>Understanding the efficacy, reliability and resiliency of computer  vision techniques for malware detection and future research directions</a></b><br>2019 - Chen, Li"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/1904.10522'>Block-distributed Gradient Boosted Trees</a></b><br>2019 - Vasiloudis, Theodore, Cho, Hyunsu, Boström, Henrik"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_query = \"machine learning\"\n",
    "example_result = and_query(example_query)\n",
    "display(HTML(f\"<h3>There are {len(example_result)} results for '{example_query}':</h3><hr>\"))\n",
    "for i in list(example_result)[:5]:\n",
    "    print_paper(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with Tokenization and Stemming\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "The paper at index 35827 is related to neural networks, yet they are not part of the results when you look for `neural network`, why is that the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is paper 35827 in the search results for 'neural network'? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/1710.05477'>A multi-branch convolutional neural network for detecting double JPEG  compression</a></b><br>2017 - Li, Bin, Luo, Hu, Zhang, Haoxin, Tan, Shunquan, Ji, Zhongzhou"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Detection of double JPEG compression is important to forensics analysis. A few methods were proposed based on convolutional <span style='background: yellow'>neural</span> <span style='background: yellow'>network</span>s (CNNs). These methods only accept inputs from pre-processed data, such as histogram features and/or decompressed images. In this paper, we present a CNN solution by using raw DCT (discrete cosine transformation) coefficients from JPEG images as input. Considering the DCT sub-band nature in JPEG, a multiple-branch CNN structure has been designed to reveal whether a JPEG format image has been doubly compressed. Comparing with previous methods, the proposed method provides end-to-end detection capability. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed <span style='background: yellow'>network</span>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"neural network\"\n",
    "i = 35827\n",
    "\n",
    "print(f\"Is paper {i} in the search results for '{query}'?\", i in and_query(query))\n",
    "print_paper(i, True, stem(tokenize(query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the word `neural network` is highlighted in the description, after `network` there is a full stop. If you change the query to `neural network.`, this paper will be in the results. This demonstrates that tokenization isn't as simple as breaking a sentence by space. Here are some examples where tokenization becomes complicated:\n",
    "\n",
    "- Den Haag\n",
    "- Lebensversicherungsgesellschaftsangestellter (life insurance company employee)\n",
    "- ziektekostenverzekeringsmaatschappij (health insurance company)\n",
    "- Languages with no spaces (e.g. Chinese, Japanese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "As you can see below, the paper with id 36090 has the word `neural network` in the title, yet it is not in the results when we search for `neural network`. Why is it the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is paper 36090 in the search results for 'neural network'? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/1710.08864'>One pixel attack for fooling deep neural networks</a></b><br>2019 - Su, Jiawei, Vargas, Danilo Vasconcellos, Kouichi, Sakurai"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of <span style='background: yellow'>network</span>s due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against <span style='background: yellow'>neural</span> <span style='background: yellow'>network</span>s for evaluating robustness."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"neural network\"\n",
    "i = 36090\n",
    "\n",
    "print(f\"Is paper {i} in the search results for '{query}'?\", i in and_query(query))\n",
    "# print(descriptions[i])\n",
    "print_paper(i, True, stem(tokenize(query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description only has `neural networks` but not `neural network`, and our stemming function does not reduce `networks` to `network`. This paper would show up if you look for `neural networks`. But how do we stem all the words that can appear in the language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved inverted index with `nltk`\n",
    "\n",
    "Luckily, there is a module [`nltk`](http://www.nltk.org/) in Python that simplifies all those challenges we see above. `nltk` should be installed when you install Anaconda, if not you can run this in a terminal to install it: `conda install -c anaconda nltk`\n",
    "\n",
    "Let's define `tokenize2()` and `stem2()` to better process the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yoyo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "nltk.download('punkt')     # Download the pre-trained Punkt tokenizer\n",
    "stemmer = EnglishStemmer() # Initialise the English stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning']\n",
      "['machin', 'learn']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize treats punctuation as tokens\n",
    "# So we check the tokens with str.isalnum() to see if they are alphanumeric\n",
    "# Obviously this only works with English\n",
    "def tokenize2(text):\n",
    "    return [t for t in set(word_tokenize(text)) if t.isalnum()]\n",
    "\n",
    "def stem2(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "sentence = \"machine learning\"\n",
    "print(stem(tokenize(sentence)))\n",
    "print(stem2(tokenize2(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will rebuild the index using the NLTK-powered functions (only a subset of 5000 papers are used since these functions are a lot slower):\n",
    "\n",
    "**Exercise:** Build another inverted index using the subset of articles and the NLTK-enhanced functions defined above and save it in `smarter_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarter_index = defaultdict(set)\n",
    "\n",
    "subset = papers[:5000]\n",
    "\n",
    "# This may take a while\n",
    "for (i, p) in enumerate(subset):\n",
    "    for term in stem2(tokenize2(p.description)):\n",
    "        smarter_index[term].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, let's rebuild the naïve inverted index using the same subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverted_index = defaultdict(set)\n",
    "\n",
    "for (i, p) in enumerate(subset):\n",
    "    for term in stem(tokenize(p.description)):\n",
    "        inverted_index[term].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the number of keys both indexes have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41897\n",
      "11874\n"
     ]
    }
   ],
   "source": [
    "print(len(inverted_index.keys()))\n",
    "print(len(smarter_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Explain the difference in the number of keys and search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The newer tokenization and stemming functions are better at splitting sentences and grouping the tokens, resulting in fewer keys and more results under a term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a new query function `and_query2(query)` using the new tokenization and stemming functions and the smarter inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_query2(query):\n",
    "    terms = stem2(tokenize2(query))\n",
    "    # Find the intersection of the terms and return it\n",
    "    results = smarter_index[terms[0]]\n",
    "    for term in terms[1:]:\n",
    "        results = results.intersection(smarter_index[term])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the search results of the two indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72 results for 'The Who' using the naïve index\n",
      "There are 80 results for 'The Who' using the smarter index\n"
     ]
    }
   ],
   "source": [
    "example_query = \"The Who\"\n",
    "example_result = and_query(example_query)\n",
    "example_result2 = and_query2(example_query)\n",
    "print(f\"There are {len(example_result)} results for '{example_query}' using the naïve index\")\n",
    "print(f\"There are {len(example_result2)} results for '{example_query}' using the smarter index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Given the nature of our dataset, how many of the papers from above do you think are actually about [The Who](http://en.wikipedia.org/wiki/The_Who)? What could you do to prevent these kind of incorrect results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Probably none of the matches is really about the band. Because we are searching the terms individually and not as a phrase and because 'the' and 'who' and both very common words in English, we get many results where the two occur, but probably not after each other (and even then probably not referring to the band)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we looked into how we can search a library of texts using an inverted index, and how we can improve the process of building the index to get more relevant results. However, this is just the beginning, there are a lot more improvements we can do. For now all results have the same \"relevance\", how do we create a ranking out of all these results? (hint: [tf-idf](http://www.tfidf.com/), [PageRank](https://www.cs.princeton.edu/~chazelle/courses/BIB/pagerank.htm)) How do we know the context of a search, or the relations between objects in real life? (Knowledge graph [[1]](https://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html), [[2]](http://ceur-ws.org/Vol-1695/paper4.pdf)) [There](https://en.wikipedia.org/wiki/Information_retrieval) [are](https://arxiv.org/list/cs.IR/recent) [fields](https://en.wikipedia.org/wiki/Natural_language_processing) dedicated to extracting information from data, and if you want to know more, [this book](https://nlp.stanford.edu/IR-book/) is a good start.\n",
    "\n",
    "All theories aside, I was hoping the exercises give you more confidence with Python, since a search engine is nothing more than a dict of terms and matching results, and the harder part is usually with understanding the problem (in this case information retrieval theory)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
