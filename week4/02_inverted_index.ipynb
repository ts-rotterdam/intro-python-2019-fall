{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple search engine\n",
    "## Part 2: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will build a inverted search index, and then search the dataset using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "For this exercise we will be loading both files from the dataset and generate a named tuple also containing the descriptions. This may take roughly a minute depending on the computing power of your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle, lzma\n",
    "from collections import namedtuple\n",
    "\n",
    "summaries_file = 'data/arxiv_cs_summaries.pickle.xz'\n",
    "descriptions_file = 'data/arxiv_cs_descriptions.pickle.xz'\n",
    "\n",
    "summaries = pickle.load(lzma.LZMAFile(summaries_file, 'rb'))\n",
    "descriptions = pickle.load(lzma.LZMAFile(descriptions_file, 'rb'))\n",
    "\n",
    "paper = namedtuple(\"paper\", [\"title\", \"authors\", \"year\", \"link\", \"description\"])\n",
    "papers = [paper(*summaries[i], descriptions[i]) for i in range(len(summaries))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper(title='Intelligent location of simultaneously active acoustic emission sources:  Part I', authors=['Kosel, T.', 'Grabec, I.'], year=2007, link='http://arxiv.org/abs/0704.0047', description='The intelligent acoustic emission locator is described in Part I, while Part II discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.   The location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. This article describes an intelligent acoustic emission source locator. The intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. Locator performance was tested on different test specimens. Tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. The location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. This is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.')\n"
     ]
    }
   ],
   "source": [
    "print(papers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The code below defines a function `print_paper(paper_id, print_description=False, highlight=[])` to print the summary of a paper (with descriptions) given its id, and optionally highlight parts of the description given a list of strings. You can use this to quickly test your search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_paper(paper_id, print_description=False, highlight=[]):\n",
    "    p = papers[paper_id]\n",
    "    display(HTML(f\"<b><a href='{p.link}'>{p.title}</a></b><br>{p.year} - {', '.join(p.authors)}\"))\n",
    "    if print_description:\n",
    "        description = descriptions[paper_id]\n",
    "        for h in highlight:\n",
    "            description = description.replace(h, f\"<span style='background: yellow'>{h}</span>\")\n",
    "        display(HTML(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0047'>Intelligent location of simultaneously active acoustic emission sources:  Part I</a></b><br>2007 - Kosel, T., Grabec, I."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0050'>Intelligent location of simultaneously active acoustic emission sources:  Part II</a></b><br>2007 - Kosel, T., Grabec, I."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0304'>The World as Evolving Information</a></b><br>2010 - Gershenson, Carlos"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in range(3):\n",
    "    print_paper(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index\n",
    "\n",
    "What is the most basic problem a search engine has to solve? Given a search term as input, output a list of results containing the search term. More specifically, given a `string` as input, return a list of papers whose descriptions contain the `string`. So how do we do that?\n",
    "\n",
    "\"The answer is quite straightforward\", you say, \"just loop over the papers and if the paper contain the result, put it in a list, something like this\":\n",
    "\n",
    "```\n",
    "def search(query):\n",
    "    return [p for p in papers if query in p.description]\n",
    "```\n",
    "\n",
    "The function above would indeed return a list of papers with the results, but there are a few problems:\n",
    "- This function only returns exact matches of your search query, i.e. `\"Computer\" != \"computer\" != \"computers\"`\n",
    "- The search function takes a while to run (and will take even longer as the dataset becomes bigger)\n",
    "\n",
    "The above reason (mostly the performance one) is why we would use an inverted index. First let's talk about the index we have learned about. We have a list of tuples, and each tuple contains the properties of a paper (title, authors, year, link, description). This list can be treated as an index: we can get one paper using its position (index) in the list.\n",
    "\n",
    "An inverted index is created differently: we create an index based on the **terms** (similar to words) within the descriptions. Instead of getting one paper by its position (i.e. `papers[12345]` returns a `tuple`), we get a list of papers containing a term by the term itself (i.e. `index[\"computer\"]` returns a `list` of papers containing the term `\"computer\"` in the description).\n",
    "\n",
    "To create an inverted index, first we process the descriptions into a list of terms, and then for each of those terms we create a set of articles containing that word. If we rephrase it into Python terms, we are creating a `dict` whose keys are the terms (`string`) and the results are a `set` of indices (`int`) of the articles whose description contains that term, for example:\n",
    "```\n",
    "{\n",
    "    \"computer\": {1, 2, 3, 4, 5},\n",
    "    \"network\": {2, 5, 8, 9},\n",
    "    \"term\": {1, 11},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text\n",
    "\n",
    "Two of the common steps in producing a list of **terms** from a text: **tokenization** and **stemming**.\n",
    "\n",
    "#### Tokenization\n",
    "Tokenization breaks a body of text into individual semantic units called **tokens**, for example:\n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "becomes\n",
    "```\n",
    "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n",
    "```\n",
    "\n",
    "#### Stemming\n",
    "Stemming reduces/normalises tokens to a base *stem* so that we don't need an exact match for the same word in another form, for example *is*, *am*, *are* can be reduced to *be*, and in the above sentence *The* can be reduced to *the*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, we will tokenize the paper descriptions by splitting the text by space, and then stem the tokens by converting them into lower case. Let's define the two functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split(\" \")\n",
    "\n",
    "def stem(tokens):\n",
    "    return [t.lower() for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# A defaultdict creates a set by defualt when we save items to a new key\n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "# This may take a while\n",
    "for (i, p) in enumerate(papers):\n",
    "    for term in stem(tokenize(p.description)):\n",
    "        inverted_index[term].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the index\n",
    "Now that we have created the index let's test it with some queries. The code below should return 38618 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>There are 30616 results for 'learning':</h3><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0047'>Intelligent location of simultaneously active acoustic emission sources:  Part I</a></b><br>2007 - Kosel, T., Grabec, I."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.0671'>Learning from compressed observations</a></b><br>2007 - Raginsky, Maxim"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.1028'>A neural network approach to ordinal regression</a></b><br>2007 - Cheng, Jianlin"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.2668'>Supervised Feature Selection via Dependence Estimation</a></b><br>2007 - Song, Le, Smola, Alex, Gretton, Arthur, Borgwardt, Karsten, Bedo, Justin"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><a href='http://arxiv.org/abs/0704.3453'>An Adaptive Strategy for the Classification of G-Protein Coupled  Receptors</a></b><br>2007 - Mohamed, S., Rubin, D., Marwala, T."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"learning\"\n",
    "\n",
    "display(HTML(f\"<h3>There are {len(inverted_index[query])} results for '{query}':</h3><hr>\"))\n",
    "\n",
    "for i in list(inverted_index[query])[:5]:\n",
    "    print_paper(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverted index we created above can be searched by a single word, but we rarely want to search with only one term. To find documents containing multiple search terms, we can look into the sets of papers for each search term and find the intersection of all of them.\n",
    "\n",
    "**Exercise:** Define a function `and_query(query)` that returns a set of articles that match all input terms. The query for `machine learning` should return 7944 results.\n",
    "\n",
    "*Hint:* Tokenize and stem your query first using `tokenize()` and `stem()`. The results in the inverted index are `set`s, which means you can perform set operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def and_query(query):\n",
    "    # Get a list of terms\n",
    "    terms = \n",
    "    # Return a set of results containing all terms of the query\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = \"machine learning\"\n",
    "example_result = and_query(example_query)\n",
    "display(HTML(f\"<h3>There are {len(example_result)} results for '{example_query}':</h3><hr>\"))\n",
    "for i in list(example_result)[:5]:\n",
    "    print_paper(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with Tokenization and Stemming\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "The paper at index 35827 is related to neural networks, yet they are not part of the results when you look for `neural network`, why is that the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"neural network\"\n",
    "i = 35827\n",
    "\n",
    "print(f\"Is paper {i} in the search results for '{query}'?\", i in and_query(query))\n",
    "print_paper(i, True, stem(tokenize(query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the word `neural network` is highlighted in the description, after `network` there is a full stop. If you change the query to `neural network.`, this paper will be in the results. This demonstrates that tokenization isn't as simple as breaking a sentence by space. Here are some examples where tokenization becomes complicated:\n",
    "\n",
    "- Den Haag\n",
    "- Lebensversicherungsgesellschaftsangestellter (life insurance company employee)\n",
    "- ziektekostenverzekeringsmaatschappij (health insurance company)\n",
    "- Languages with no spaces (e.g. Chinese, Japanese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "As you can see below, the paper with id 36090 has the word `neural network` in the title, yet it is not in the results when we search for `neural network`. Why is it the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query = \"neural network\"\n",
    "i = 36090\n",
    "\n",
    "print(f\"Is paper {i} in the search results for '{query}'?\", i in and_query(query))\n",
    "# print(descriptions[i])\n",
    "print_paper(i, True, stem(tokenize(query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description only has `neural networks` but not `neural network`, and our stemming function does not reduce `networks` to `network`. This paper would show up if you look for `neural networks`. But how do we stem all the words that can appear in the language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved inverted index with `nltk`\n",
    "\n",
    "Luckily, there is a module [`nltk`](http://www.nltk.org/) in Python that simplifies all those challenges we see above. `nltk` should be installed when you install Anaconda, if not you can run this in a terminal to install it: `conda install -c anaconda nltk`\n",
    "\n",
    "Let's define `tokenize2()` and `stem2()` to better process the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "nltk.download('punkt')     # Download the pre-trained Punkt tokenizer\n",
    "stemmer = EnglishStemmer() # Initialise the English stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word_tokenize treats punctuation as tokens\n",
    "# So we check the tokens with str.isalnum() to see if they are alphanumeric\n",
    "# Obviously this only works with English\n",
    "def tokenize2(text):\n",
    "    return [t for t in set(word_tokenize(text)) if t.isalnum()]\n",
    "\n",
    "def stem2(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(stem(tokenize(sentence)))\n",
    "print(stem2(tokenize2(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will rebuild the index using the NLTK-powered functions (only a subset of 5000 papers are used since these functions are a lot slower):\n",
    "\n",
    "**Exercise:** Build another inverted index using the subset of articles and the NLTK-enhanced functions defined above and save it in `smarter_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smarter_index = defaultdict(set)\n",
    "\n",
    "subset = papers[:5000]\n",
    "\n",
    "# Build the smarter_index below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, let's rebuild the naïve inverted index using the same subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverted_index = defaultdict(set)\n",
    "\n",
    "for (i, p) in enumerate(subset):\n",
    "    for term in stem(tokenize(p.description)):\n",
    "        inverted_index[term].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the number of keys both indexes have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inverted_index.keys()))\n",
    "print(len(smarter_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Explain the difference in the number of keys and search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** [Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a new query function `and_query2(query)` using the new tokenization and stemming functions and the smarter inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_query2(query):\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the search results of the two indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = \"The Who\"\n",
    "example_result = and_query(example_query)\n",
    "example_result2 = and_query2(example_query)\n",
    "print(f\"There are {len(example_result)} results for '{example_query}' using the naïve index\")\n",
    "print(f\"There are {len(example_result2)} results for '{example_query}' using the smarter index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Given the nature of our dataset, how many of the papers from above do you think are actually about [The Who](http://en.wikipedia.org/wiki/The_Who)? What could you do to prevent these kind of incorrect results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** [Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we looked into how we can search a library of texts using an inverted index, and how we can improve the process of building the index to get more relevant results. However, this is just the beginning, there are a lot more improvements we can do. For now all results have the same \"relevance\", how do we create a ranking out of all these results? (hint: [tf-idf](http://www.tfidf.com/), [PageRank](https://www.cs.princeton.edu/~chazelle/courses/BIB/pagerank.htm)) How do we know the context of a search, or the relations between objects in real life? (Knowledge graph [[1]](https://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html), [[2]](http://ceur-ws.org/Vol-1695/paper4.pdf)) [There](https://en.wikipedia.org/wiki/Information_retrieval) [are](https://arxiv.org/list/cs.IR/recent) [fields](https://en.wikipedia.org/wiki/Natural_language_processing) dedicated to extracting information from data, and if you want to know more, [this book](https://nlp.stanford.edu/IR-book/) is a good start.\n",
    "\n",
    "All theories aside, I was hoping the exercises give you more confidence with Python, since a search engine is nothing more than a dict of terms and matching results, and the harder part is usually with understanding the problem (in this case information retrieval theory)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
